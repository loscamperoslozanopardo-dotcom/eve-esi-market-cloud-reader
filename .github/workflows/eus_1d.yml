name: EVE Universe Solar Systems 11:07 UTC

on:
  schedule:
    # 11:07 UTC every day
    - cron: "7 11 * * *"
  workflow_dispatch:

permissions:
  contents: read
  id-token: write

concurrency:
  group: eus_1d
  cancel-in-progress: false

jobs:
  load:
    runs-on: ubuntu-latest
    timeout-minutes: 25

    env:
      # BigQuery
      BQ_PROJECT_ID: eve-market-sandbox
      BQ_DATASET_ID: eve_market
      BQ_LOCATION: EU

      # WIF (mismo pool/provider que ya usas en EUR)
      WIF_PROVIDER: projects/791118475780/locations/global/workloadIdentityPools/eve-market-gh-poolv2/providers/github
      SERVICE_ACCOUNT: eve-market-bq-loader@eve-market-sandbox.iam.gserviceaccount.com

      # ESI
      ESI_BASE_URL: https://esi.evetech.net
      ESI_DATASOURCE: tranquility
      # Si no existe el secret, usamos un UA por defecto (evita UA vacío)
      ESI_USER_AGENT: ${{ secrets.ESI_USER_AGENT || 'eve-esi-market-cloud-reader (github: loscamperoslozanopardo-dotcom/eve-esi-market-cloud-reader)' }}

      # Estado persistente (cache)
      EUS_STATE_PATH: state/eus_hdrs.json

      # Concurrencia / rate limit (tu script ya aplica rate global)
      EUS_WORKERS: "20"
      EUS_QPS: "20"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Prepare folders
        run: |
          mkdir -p state out
          if [ ! -f "${EUS_STATE_PATH}" ]; then echo '{}' > "${EUS_STATE_PATH}"; fi

      - name: Restore state cache (headers)
        uses: actions/cache@v4
        with:
          path: state/eus_hdrs.json
          # caches son inmutables; key único por run, restore por prefijo
          key: eus-hdrs-${{ github.run_id }}
          restore-keys: |
            eus-hdrs-

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install google-cloud-bigquery
          python -c "from google.cloud import bigquery; print('bigquery ok:', bigquery.__version__)"

      - name: Auth to Google Cloud (WIF)
        uses: google-github-actions/auth@v3
        with:
          project_id: ${{ env.BQ_PROJECT_ID }}
          workload_identity_provider: ${{ env.WIF_PROVIDER }}
          service_account: ${{ env.SERVICE_ACCOUNT }}
          create_credentials_file: true

      - name: Run loader (1 + 3 retries, 10 min apart)
        shell: bash
        run: |
          set -euo pipefail

          max_attempts=4   # 1 + 3 retries
          attempt=1

          : > out/eus_run.log

          while [ "$attempt" -le "$max_attempts" ]; do
            echo "Attempt $attempt/$max_attempts..." | tee -a out/eus_run.log
            if python src/eus_ld.py 2>&1 | tee -a out/eus_run.log; then
              echo "SUCCESS" | tee -a out/eus_run.log
              exit 0
            fi

            if [ "$attempt" -eq "$max_attempts" ]; then
              echo "All attempts failed." | tee -a out/eus_run.log
              exit 1
            fi

            echo "Attempt $attempt failed; sleeping 600s then retry..." | tee -a out/eus_run.log
            sleep 600
            attempt=$((attempt+1))
          done

      # No artifacts on success; only upload minimal debug info if something failed.
      - name: Upload debug artifact (only on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: eus-debug
          path: |
            out/eus_run.log
            state/eus_hdrs.json
          retention-days: 2

