name: EUR 1d

on:
  schedule:
    - cron: "7 11 * * *" # cada día 11:07 UTC
  workflow_dispatch:

concurrency:
  group: eur-1d
  cancel-in-progress: false

permissions:
  contents: read
  id-token: write

jobs:
  eur_task:
    runs-on: ubuntu-latest
    timeout-minutes: 25

    env:
      # BigQuery
      PROJECT_ID: eve-market-sandbox
      PROJECT_NUMBER: "791118475780"
      LOCATION: EU
      DATASET_ID: eve_market
      TABLE_REGIONS: regions
      TABLE_REGIONS_DT: regions_dt

      # WIF (mismo pool/provider que ya usas en otros workflows)
      WIF_PROVIDER: projects/791118475780/locations/global/workloadIdentityPools/eve-market-gh-poolv2/providers/github
      SERVICE_ACCOUNT: eve-market-bq-loader@eve-market-sandbox.iam.gserviceaccount.com

      # ESI
      ESI_BASE: https://esi.evetech.net/latest
      ESI_DATASOURCE: tranquility
      ESI_LANGUAGE: en
      USER_AGENT: "eve-esi-market-cloud-reader (github: loscamperoslozanopardo-dotcom/eve-esi-market-cloud-reader)"

      # Estado persistente (cache)
      STATE_FILE: state/eur_hdrs.json

      # Robustez HTTP
      TIMEOUT_S: "60"
      MAX_RETRIES: "5"
      ERROR_LIMIT_THRESHOLD: "20"
      WORKERS: "10"

      # Política: solo re-verificar cuando Expires haya vencido
      FORCE_VERIFY_AFTER_EXPIRES: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Prepare folders
        run: |
          mkdir -p state out
          if [ ! -f "${STATE_FILE}" ]; then echo '{}' > "${STATE_FILE}"; fi

      - name: Restore state cache (headers)
        uses: actions/cache@v4
        with:
          path: state/eur_hdrs.json
          # caches son inmutables; key único por run, restore por prefijo
          key: eur-hdrs-${{ github.run_id }}
          restore-keys: |
            eur-hdrs-

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # requerido por src/eur_ld.py (import google.cloud.bigquery)
          pip install google-cloud-bigquery
          python -c "from google.cloud import bigquery; print('bigquery ok:', bigquery.__version__)"

      - name: Auth to Google Cloud (WIF)
        uses: google-github-actions/auth@v3
        with:
          project_id: ${{ env.PROJECT_ID }}
          workload_identity_provider: ${{ env.WIF_PROVIDER }}
          service_account: ${{ env.SERVICE_ACCOUNT }}
          create_credentials_file: true

      - name: Run EUR (3 retries, 10m apart)
        shell: bash
        run: |
          set -euo pipefail

          max_attempts=4   # 1 + 3 retries
          attempt=1

          : > out/eur_run.log

          while [ "$attempt" -le "$max_attempts" ]; do
            echo "Attempt $attempt/$max_attempts..." | tee -a out/eur_run.log
            if python src/eur_ld.py 2>&1 | tee -a out/eur_run.log; then
              exit 0
            fi

            if [ "$attempt" -eq "$max_attempts" ]; then
              echo "All attempts failed." | tee -a out/eur_run.log
              exit 1
            fi

            echo "Attempt $attempt failed; sleeping 600s then retry..." | tee -a out/eur_run.log
            sleep 600
            attempt=$((attempt+1))
          done

      - name: Upload debug artifact (only on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: eur-debug
          path: |
            out/eur_run.log
            state/eur_hdrs.json
          retention-days: 2
